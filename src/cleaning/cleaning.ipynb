{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Import required packages and libraries for data exploration\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import pyabsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up file path and data handling objects\n",
    "'''\n",
    "PATH = \"../data/reviews.csv\"\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Data Points\n",
    "The first stage of data cleaning is to identify and remove data points that aren't related to our task. In \"Amazon Fine Food Reviews\", we have many different product reviews including: pet food, medicine, microwavable food, fine foods, etc.\n",
    "- Is this category of food or type of review relevant to our task?\n",
    "- Would removing this type of review from the data improve the accuracy of our model?\n",
    "- If we remove this type of review, how will it effect our training process (would there be too little data remaining?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Uncecessary Columns\n",
    "- What columns are necessary for our model? \n",
    "- Is there anything that needs to be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include features that can be plotted in correlation matrix\n",
    "# String features cannot be intepreted in correlation matrix\n",
    "numeric_data = data.drop(columns=[\"ProductId\", \"UserId\", \"ProfileName\", \"Summary\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the helpfulness\n",
    "helpfulness_scores = data[\"HelpfulnessNumerator\"]/data[\"HelpfulnessDenominator\"].replace(0,np.nan)\n",
    "\n",
    "# Add the new helpfulness column to the numeric data as correlation feature\n",
    "data[\"Helpfulness\"] = helpfulness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Helpfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>5</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>2</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>5</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductId          UserId  Score                             Summary  \\\n",
       "0       B001E4KFG0  A3SGXH7AUHU8GW      5               Good Quality Dog Food   \n",
       "1       B00813GRG4  A1D87F6ZCVE5NK      1                   Not as Advertised   \n",
       "2       B000LQOCH0   ABXLMWJIXXAIN      4               \"Delight\" says it all   \n",
       "3       B000UA0QIQ  A395BORC6FGVXV      2                      Cough Medicine   \n",
       "4       B006K2ZZ7K  A1UQRSCLF8GW1T      5                         Great taffy   \n",
       "...            ...             ...    ...                                 ...   \n",
       "568449  B001EO7N10  A28KG5XORO54AY      5                 Will not do without   \n",
       "568450  B003S1WTCU  A3I8AFVPEE8KI5      2                        disappointed   \n",
       "568451  B004I613EE  A121AA1GQV751Z      5            Perfect for our maltipoo   \n",
       "568452  B004I613EE   A3IBEVCTXKNOH      5  Favorite Training and reward treat   \n",
       "568453  B001LR2CU2  A3LGQPJCZVL9UC      5                         Great Honey   \n",
       "\n",
       "                                                     Text  Helpfulness  \n",
       "0       I have bought several of the Vitality canned d...          1.0  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...          NaN  \n",
       "2       This is a confection that has been around a fe...          1.0  \n",
       "3       If you are looking for the secret ingredient i...          1.0  \n",
       "4       Great taffy at a great price.  There was a wid...          NaN  \n",
       "...                                                   ...          ...  \n",
       "568449  Great for sesame chicken..this is a good if no...          NaN  \n",
       "568450  I'm disappointed with the flavor. The chocolat...          NaN  \n",
       "568451  These stars are small, so you can give 10-15 o...          1.0  \n",
       "568452  These are the BEST treats for training and rew...          1.0  \n",
       "568453  I am very satisfied ,product is as advertised,...          NaN  \n",
       "\n",
       "[568454 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As seen in the data exploration stage, most numerical features excluding \n",
    "# the newly created \"Helpfulness\" were not indicative of Score\n",
    "data.drop(columns=[\n",
    "    \"Id\", \n",
    "    \"ProfileName\", \n",
    "    \"HelpfulnessNumerator\", \n",
    "    \"HelpfulnessDenominator\",\n",
    "    \"Time\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Sensitivity\n",
    "Convert the input features in the raw dataset into a case insensitive format (all lowercase/uppercase) to reduce the amount of distinct words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Filler Words\n",
    "Some words like \"I\", \"the\", \"a\", etc. don't impact the sentiment of the text content. Remove these words from all review content so there is less redundant features for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Handling\n",
    "Some words that contain punctuation can be recorded as separate features without punctuation handling (e.g., \"Steve's pizza is great!\" and \"Steve makes great pizza!\").\n",
    "\n",
    "| is | great | great! | makes | pizza | pizza! | Steve | Steve's |\n",
    "|----|-------|--------|-------|-------|--------|-------|---------|\n",
    "|1   | 1     | 1      | 1     | 1     | 1      | 1     | 1       |\n",
    "\n",
    "We want to remove uncessesary punctuation so that we don't have duplicates of effectively the same word.\n",
    "| is | great | makes | pizza | Steve |\n",
    "|----|-------|-------|-------|-------|\n",
    "| 1  | 2     | 1     | 2     | 2     |\n",
    "\n",
    "Doing this prevents our model from interpreting duplicate words as two separate features and reduces the number of dimensions our model has to process (increasing efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Split\n",
    "In this section we need to split the dataset into single entity and multiple entity data points. This step is necessary because the framework for our model requires that single entity data points are handled by **model A** and multiple entity data points are handled by **model B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 20:30:20] (2.4.1.post1) Please specify the task code, e.g. from pyabsa import TaskCodeOption\n",
      "[2025-05-05 20:30:21] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-05 20:30:21] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-05 20:30:21] (2.4.1.post1) Checkpoint:fast_lcf_atepc is not found, you can raise an issue for requesting shares of checkpoints\n",
      "[2025-05-05 20:30:22] (2.4.1.post1) No checkpoint found in Model Hub for task: fast_lcf_atepc\n",
      "FindFile Warning --> multiple targets ['checkpoints\\\\ATEPC_ENGLISH_CHECKPOINT\\\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\\\fast_lcf_atepc.config', 'checkpoints\\\\ATEPC_MULTILINGUAL_CHECKPOINT\\\\fast_lcf_atepc.config'] found, only return the shortest path: <checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.config>\n",
      "FindFile Warning --> multiple targets ['checkpoints\\\\ATEPC_ENGLISH_CHECKPOINT\\\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\\\fast_lcf_atepc.config', 'checkpoints\\\\ATEPC_MULTILINGUAL_CHECKPOINT\\\\fast_lcf_atepc.config'] found, only return the shortest path: <checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.config>\n",
      "[2025-05-05 20:30:25] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\n",
      "[2025-05-05 20:30:25] (2.4.1.post1) config: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.config\n",
      "[2025-05-05 20:30:25] (2.4.1.post1) state_dict: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.state_dict\n",
      "[2025-05-05 20:30:25] (2.4.1.post1) model: None\n",
      "[2025-05-05 20:30:25] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.tokenizer\n",
      "[2025-05-05 20:30:29] (2.4.1.post1) Set Model Device: cpu\n",
      "[2025-05-05 20:30:29] (2.4.1.post1) Device Name: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 20:34:03] (2.4.1.post1) The results of aspect term extraction have been saved in d:\\Documents\\OneDrive - UTS\\2025\\49275 - Neural Networks and Fuzzy Logic\\Project\\Amazon-Sentiment-Analysis\\src\\cleaning\\absa\\Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n",
      "[2025-05-05 20:34:03] (2.4.1.post1) Example 0: Good Quality Dog Food\n",
      "[2025-05-05 20:34:03] (2.4.1.post1) Example 1: Not as Advertised\n",
      "[2025-05-05 20:34:03] (2.4.1.post1) Example 2: \" <Delight:Positive Confidence:0.9965> \" says it all\n",
      "[2025-05-05 20:34:03] (2.4.1.post1) Example 3: Cough Medicine\n",
      "[2025-05-05 20:34:03] (2.4.1.post1) Example 4: Great taffy\n",
      "Item 1 aspects: ['Dog Food']\n",
      "Item 2 aspects: ['Advertised']\n",
      "Item 3 aspects: ['Delight']\n",
      "Item 4 aspects: []\n",
      "Item 5 aspects: ['taffy']\n"
     ]
    }
   ],
   "source": [
    "from pyabsa import AspectTermExtraction as ATEPC, available_checkpoints\n",
    "\n",
    "# view available checkpoints\n",
    "checkpoint_map = available_checkpoints()\n",
    "\n",
    "# load model\n",
    "aspect_extractor = ATEPC.AspectExtractor(\n",
    "    checkpoint='fast_lcf_atepc',\n",
    "    auto_device=True,\n",
    "    cal_perplexity=True\n",
    ")\n",
    "\n",
    "# single sentence prediction\n",
    "results = aspect_extractor.predict(\n",
    "    data.iloc[:5][\"Summary\"].tolist(),\n",
    "    print_result=True,\n",
    "    ignore_error=True,\n",
    ")\n",
    "\n",
    "# Print aspect terms for each item\n",
    "for i, result in enumerate(results):\n",
    "    aspects = result.get(\"aspect\", [])\n",
    "    print(f\"Item {i+1} aspects: {aspects}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
