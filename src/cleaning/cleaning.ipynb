{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import required packages and libraries for data exploration\n",
    "'''\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up file path and data handling objects\n",
    "'''\n",
    "PATH = \"../data/reviews.csv\"\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Data Points\n",
    "The first stage of data cleaning is to identify and remove data points that aren't related to our task. In \"Amazon Fine Food Reviews\", we have many different product reviews including: pet food, medicine, microwavable food, fine foods, etc.\n",
    "- Is this category of food or type of review relevant to our task?\n",
    "- Would removing this type of review from the data improve the accuracy of our model?\n",
    "- If we remove this type of review, how will it effect our training process (would there be too little data remaining?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Uncecessary Columns\n",
    "- What columns are necessary for our model? \n",
    "- Is there anything that needs to be removed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Sensitivity\n",
    "Convert the input features in the raw dataset into a case insensitive format (all lowercase/uppercase) to reduce the amount of distinct words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of converted Summary:\n",
      "0    good quality dog food\n",
      "1        not as advertised\n",
      "2    \"delight\" says it all\n",
      "3           cough medicine\n",
      "4              great taffy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Sample of converted Text:\n",
      "0    i have bought several of the vitality canned d...\n",
      "1    product arrived labeled as jumbo salted peanut...\n",
      "2    this is a confection that has been around a fe...\n",
      "3    if you are looking for the secret ingredient i...\n",
      "4    great taffy at a great price.  there was a wid...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert text features to lowercase\n",
    "data['Summary'] = data['Summary'].str.lower()\n",
    "data['Text'] = data['Text'].str.lower()\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"Sample of converted Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of converted Text:\")\n",
    "print(data['Text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Filler Words\n",
    "Some words like \"I\", \"the\", \"a\", etc. don't impact the sentiment of the text content. Remove these words from all review content so there is less redundant features for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "\n",
      "Sample of English stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "Removing stop words from Summary...\n",
      "Removing stop words from Text...\n",
      "\n",
      "Sample of processed Summary:\n",
      "0    good quality dog food\n",
      "1               advertised\n",
      "2           \"delight\" says\n",
      "3           cough medicine\n",
      "4              great taffy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Sample of processed Text:\n",
      "0    bought several vitality canned dog food produc...\n",
      "1    product arrived labeled jumbo salted peanuts.....\n",
      "2    confection around centuries. light, pillowy ci...\n",
      "3    looking secret ingredient robitussin believe f...\n",
      "4    great taffy great price. wide assortment yummy...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print sample of stopwords\n",
    "print(\"\\nSample of English stopwords:\")\n",
    "print(sorted(list(stop_words))[:10])  # Print first 10 stopwords\n",
    "\n",
    "def remove_stopwords_from_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Process both columns\n",
    "print(\"Removing stop words from Summary...\")\n",
    "data['Summary'] = data['Summary'].apply(remove_stopwords_from_text)\n",
    "\n",
    "print(\"Removing stop words from Text...\")\n",
    "data['Text'] = data['Text'].apply(remove_stopwords_from_text)\n",
    "\n",
    "# Print samples for verification\n",
    "print(\"\\nSample of processed Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of processed Text:\")\n",
    "print(data['Text'].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Handling\n",
    "Some words that contain punctuation can be recorded as separate features without punctuation handling (e.g., \"Steve's pizza is great!\" and \"Steve makes great pizza!\").\n",
    "\n",
    "| is | great | great! | makes | pizza | pizza! | Steve | Steve's |\n",
    "|----|-------|--------|-------|-------|--------|-------|---------|\n",
    "|1   | 1     | 1      | 1     | 1     | 1      | 1     | 1       |\n",
    "\n",
    "We want to remove uncessesary punctuation so that we don't have duplicates of effectively the same word.\n",
    "| is | great | makes | pizza | Steve |\n",
    "|----|-------|-------|-------|-------|\n",
    "| 1  | 2     | 1     | 2     | 2     |\n",
    "\n",
    "Doing this prevents our model from interpreting duplicate words as two separate features and reduces the number of dimensions our model has to process (increasing efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing punctuation from Summary...\n",
      "Removing punctuation from Text...\n",
      "\n",
      "Sample of processed Summary:\n",
      "0    good quality dog food\n",
      "1               advertised\n",
      "2             delight says\n",
      "3           cough medicine\n",
      "4              great taffy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Sample of processed Text:\n",
      "0    bought several vitality canned dog food produc...\n",
      "1    product arrived labeled jumbo salted peanutsth...\n",
      "2    confection around centuries light pillowy citr...\n",
      "3    looking secret ingredient robitussin believe f...\n",
      "4    great taffy great price wide assortment yummy ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "    \n",
    "# Function to remove punctuation from text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Remove other punctuation\n",
    "    text = ''.join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    # Handle multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "    \n",
    "# Process both columns\n",
    "print(\"Removing punctuation from Summary...\")\n",
    "data['Summary'] = data['Summary'].apply(clean_text)\n",
    "\n",
    "print(\"Removing punctuation from Text...\")\n",
    "data['Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "    # Print samples for verification\n",
    "print(\"\\nSample of processed Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of processed Text:\")\n",
    "print(data['Text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Split\n",
    "In this section we need to split the dataset into single entity and multiple entity data points. This step is necessary because the framework for our model requires that single entity data points are handled by **model A** and multiple entity data points are handled by **model B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary field...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summary batches: 100%|██████████| 285/285 [07:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Text field...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text batches:   4%|▍         | 11/285 [02:18<1:00:01, 13.14s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def create_aspect_patterns(nlp):\n",
    "    food_aspects = {\n",
    "        'taste': [\n",
    "            'taste', 'flavor', 'flavour', 'delicious', 'yummy', 'tasty', 'sweet', 'sour', 'bitter', 'spicy',\n",
    "            'savory', 'savoury', 'tangy', 'zesty', 'bland', 'flavorless', 'tasteless', 'seasoned', 'seasoning',\n",
    "            'aroma', 'fragrant', 'pungent', 'mild', 'strong', 'rich', 'subtle', 'intense', 'overpowering'\n",
    "        ],\n",
    "        'delivery': [\n",
    "            'delivery', 'shipping', 'arrived', 'packed', 'package', 'ship', 'deliver', 'transit', 'shipped',\n",
    "            'received', 'arrival', 'dispatch', 'tracking', 'courier', 'postage', 'mail', 'express', 'standard',\n",
    "            'overnight', 'next day', 'same day', 'delivery time', 'shipping time', 'arrival time'\n",
    "        ],\n",
    "        'quality': [\n",
    "            'quality', 'fresh', 'freshness', 'premium', 'organic', 'natural', 'authentic', 'genuine', 'pure',\n",
    "            'high quality', 'low quality', 'inferior', 'superior', 'excellent', 'poor', 'grade', 'standard',\n",
    "            'artisanal', 'handmade', 'crafted', 'processed', 'preserved', 'artificial', 'synthetic', 'chemical'\n",
    "        ],\n",
    "        'price': [\n",
    "            'price', 'cost', 'expensive', 'cheap', 'affordable', 'value', 'worth', 'budget', 'economical',\n",
    "            'overpriced', 'underpriced', 'reasonable', 'unreasonable', 'discount', 'sale', 'deal', 'bargain',\n",
    "            'premium', 'luxury', 'budget-friendly', 'cost-effective', 'pricey', 'price point', 'price range'\n",
    "        ],\n",
    "        'packaging': [\n",
    "            'packaging', 'container', 'box', 'seal', 'wrapped', 'packed', 'sealed', 'vacuum', 'airtight',\n",
    "            'resealable', 'leakproof', 'spillproof', 'fragile', 'damaged', 'broken', 'intact', 'secure',\n",
    "            'presentation', 'presented', 'wrapped', 'unwrapped', 'opened', 'closed', 'lid', 'cap', 'seal'\n",
    "        ],\n",
    "        'service': [\n",
    "            'service', 'customer service', 'support', 'help', 'response', 'assistance', 'staff', 'representative',\n",
    "            'agent', 'helpful', 'unhelpful', 'responsive', 'unresponsive', 'friendly', 'unfriendly', 'professional',\n",
    "            'unprofessional', 'attentive', 'inattentive', 'efficient', 'inefficient', 'satisfactory', 'unsatisfactory'\n",
    "        ],\n",
    "        'texture': [\n",
    "            'texture', 'crunchy', 'soft', 'hard', 'smooth', 'creamy', 'crispy', 'chewy', 'tender', 'tough',\n",
    "            'moist', 'dry', 'juicy', 'watery', 'thick', 'thin', 'grainy', 'gritty', 'sandy', 'powdery',\n",
    "            'flaky', 'crumbly', 'dense', 'light', 'airy', 'spongy', 'rubbery', 'gelatinous', 'sticky', 'gooey'\n",
    "        ],\n",
    "        'appearance': [\n",
    "            'appearance', 'look', 'color', 'colour', 'shape', 'size', 'presentation', 'presented', 'visual',\n",
    "            'attractive', 'unattractive', 'appealing', 'unappealing', 'beautiful', 'ugly', 'perfect', 'imperfect',\n",
    "            'uniform', 'inconsistent', 'consistent', 'varied', 'variety', 'selection', 'assortment', 'arrangement'\n",
    "        ],\n",
    "        'health': [\n",
    "            'healthy', 'unhealthy', 'nutrition', 'nutritious', 'calorie', 'calories', 'diet', 'dietary',\n",
    "            'organic', 'natural', 'artificial', 'preservative', 'additive', 'ingredient', 'allergen', 'allergy',\n",
    "            'gluten', 'dairy', 'vegan', 'vegetarian', 'kosher', 'halal', 'sugar', 'fat', 'protein', 'fiber',\n",
    "            'vitamin', 'mineral', 'nutrient', 'benefit', 'harmful', 'safe', 'unsafe'\n",
    "        ],\n",
    "        'shelf_life': [\n",
    "            'expiry', 'expiration', 'expired', 'fresh', 'stale', 'spoiled', 'rotten', 'moldy', 'mouldy',\n",
    "            'shelf life', 'storage', 'stored', 'preserve', 'preserved', 'last', 'lasts', 'durability',\n",
    "            'durable', 'perishable', 'non-perishable', 'long-lasting', 'short-lived', 'best before', 'use by'\n",
    "        ],\n",
    "        'portion': [\n",
    "            'portion', 'serving', 'size', 'amount', 'quantity', 'adequate', 'inadequate', 'sufficient',\n",
    "            'insufficient', 'generous', 'small', 'large', 'enough', 'not enough', 'plenty', 'scant',\n",
    "            'satisfying', 'unsatisfying', 'filling', 'not filling', 'substantial', 'meager', 'ample'\n",
    "        ],\n",
    "        'preparation': [\n",
    "            'prepare', 'preparation', 'cook', 'cooking', 'heat', 'heated', 'microwave', 'oven', 'stove',\n",
    "            'ready', 'ready-to-eat', 'instant', 'quick', 'easy', 'difficult', 'convenient', 'inconvenient',\n",
    "            'time-consuming', 'effortless', 'simple', 'complex', 'instructions', 'directions', 'recipe'\n",
    "        ]\n",
    "    }\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = []\n",
    "    \n",
    "    for aspect, keywords in food_aspects.items():\n",
    "        patterns.extend([nlp(text) for text in keywords])\n",
    "        matcher.add(aspect, patterns)\n",
    "        patterns = []  # Reset patterns for next aspect\n",
    "        \n",
    "    return matcher\n",
    "\n",
    "def count_aspects_batch(texts, nlp, matcher):\n",
    "    # Filter out None/NaN values\n",
    "    valid_texts = [str(text) for text in texts if pd.notna(text)]\n",
    "    if not valid_texts:\n",
    "        return [0] * len(texts)\n",
    "    \n",
    "    # Process texts in batch using spaCy's pipe\n",
    "    docs = list(nlp.pipe(valid_texts, batch_size=len(texts)))\n",
    "    \n",
    "    # Count aspects for each document\n",
    "    counts = []\n",
    "    doc_idx = 0\n",
    "    for text in texts:\n",
    "        if pd.isna(text):\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            doc = docs[doc_idx]\n",
    "            matches = matcher(doc)\n",
    "            aspects_found = set()\n",
    "            for match_id, start, end in matches:\n",
    "                aspects_found.add(nlp.vocab.strings[match_id])\n",
    "            counts.append(len(aspects_found))\n",
    "            doc_idx += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = create_aspect_patterns(nlp)\n",
    "\n",
    "batch_size = 2000\n",
    "total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "\n",
    "\n",
    "# Process in batches\n",
    "start_time = time.time()\n",
    "\n",
    "# Process Summary column\n",
    "print(\"\\nProcessing Summary field...\")\n",
    "summary_counts = []\n",
    "with tqdm(total=total_batches, desc=\"Summary batches\") as pbar:\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data['Summary'].iloc[i:i+batch_size]\n",
    "        counts = count_aspects_batch(batch, nlp, matcher)\n",
    "        summary_counts.extend(counts)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Process Text column\n",
    "print(\"\\nProcessing Text field...\")\n",
    "text_counts = []\n",
    "with tqdm(total=total_batches, desc=\"Text batches\") as pbar:\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data['Text'].iloc[i:i+batch_size]\n",
    "        counts = count_aspects_batch(batch, nlp, matcher)\n",
    "        text_counts.extend(counts)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Print performance metrics\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
    "print(f\"Average processing speed: {len(data)/processing_time:.2f} reviews/second\")\n",
    "\n",
    "# Add counts to DataFrame\n",
    "data['summary_aspect_count'] = summary_counts\n",
    "data['text_aspect_count'] = text_counts\n",
    "\n",
    "# Split the data\n",
    "print(\"\\nSplitting data into single and multiple aspect sets...\")\n",
    "\n",
    "# Single aspect: either Summary or Text mentions exactly one aspect\n",
    "single_aspect_mask = (data['summary_aspect_count'] <= 1) | (data['text_aspect_count'] <= 1)\n",
    "single_aspect_data = data[single_aspect_mask].copy()\n",
    "\n",
    "# Multiple aspect: both Summary and Text mention more than one aspect\n",
    "multiple_aspect_mask = (data['summary_aspect_count'] > 1) & (data['text_aspect_count'] > 1)\n",
    "multiple_aspect_data = data[multiple_aspect_mask].copy()\n",
    "\n",
    "\n",
    "# Remove temporary columns before saving\n",
    "single_aspect_data = single_aspect_data.drop(['summary_aspect_count', 'text_aspect_count'], axis=1)\n",
    "multiple_aspect_data = multiple_aspect_data.drop(['summary_aspect_count', 'text_aspect_count'], axis=1)\n",
    "\n",
    "# Save the split datasets\n",
    "single_entity_file = \"../data/reviews_single_aspect.csv\"\n",
    "multiple_entity_file = \"../data/reviews_multiple_aspect.csv\"\n",
    "print(f\"Saving single aspect data to {single_entity_file}...\")\n",
    "single_aspect_data.to_csv(single_entity_file, index=False)\n",
    "\n",
    "print(f\"Saving multiple aspect data to {multiple_entity_file}...\")\n",
    "multiple_aspect_data.to_csv(multiple_entity_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
