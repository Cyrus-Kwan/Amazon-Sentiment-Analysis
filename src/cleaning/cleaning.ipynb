{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import required packages and libraries for data exploration\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up file path and data handling objects\n",
    "'''\n",
    "PATH = \"../data/reviews.csv\"\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.00000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>5.684540e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>1.743817</td>\n",
       "      <td>2.22881</td>\n",
       "      <td>4.183199</td>\n",
       "      <td>1.296257e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164098.679298</td>\n",
       "      <td>7.636513</td>\n",
       "      <td>8.28974</td>\n",
       "      <td>1.310436</td>\n",
       "      <td>4.804331e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142114.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.271290e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.311120e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>426340.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332720e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>923.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  568454.000000         568454.000000            568454.00000   \n",
       "mean   284227.500000              1.743817                 2.22881   \n",
       "std    164098.679298              7.636513                 8.28974   \n",
       "min         1.000000              0.000000                 0.00000   \n",
       "25%    142114.250000              0.000000                 0.00000   \n",
       "50%    284227.500000              0.000000                 1.00000   \n",
       "75%    426340.750000              2.000000                 2.00000   \n",
       "max    568454.000000            866.000000               923.00000   \n",
       "\n",
       "               Score          Time  \n",
       "count  568454.000000  5.684540e+05  \n",
       "mean        4.183199  1.296257e+09  \n",
       "std         1.310436  4.804331e+07  \n",
       "min         1.000000  9.393408e+08  \n",
       "25%         4.000000  1.271290e+09  \n",
       "50%         5.000000  1.311120e+09  \n",
       "75%         5.000000  1.332720e+09  \n",
       "max         5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Sensitivity\n",
    "Convert the input features in the raw dataset into a case insensitive format (all lowercase/uppercase) to reduce the amount of distinct words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values from tokenizer strings\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\")\n",
    "data[\"Text\"] = data[\"Text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"delight\" says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy at a great price.  there was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  good quality dog food  i have bought several of the vitality canned d...  \n",
       "1      not as advertised  product arrived labeled as jumbo salted peanut...  \n",
       "2  \"delight\" says it all  this is a confection that has been around a fe...  \n",
       "3         cough medicine  if you are looking for the secret ingredient i...  \n",
       "4            great taffy  great taffy at a great price.  there was a wid...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all words to lowercase to reduce the number of unique features\n",
    "data[\"Summary\"] = data[\"Summary\"].str.lower()\n",
    "data[\"Text\"] = data[\"Text\"].str.lower()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Handling\n",
    "Some words that contain punctuation can be recorded as separate features without punctuation handling (e.g., \"Steve's pizza is great!\" and \"Steve makes great pizza!\").\n",
    "\n",
    "| is | great | great! | makes | pizza | pizza! | Steve | Steve's |\n",
    "|----|-------|--------|-------|-------|--------|-------|---------|\n",
    "|1   | 1     | 1      | 1     | 1     | 1      | 1     | 1       |\n",
    "\n",
    "We want to remove uncessesary punctuation so that we don't have duplicates of effectively the same word.\n",
    "| is | great | makes | pizza | Steve |\n",
    "|----|-------|-------|-------|-------|\n",
    "| 1  | 2     | 1     | 2     | 2     |\n",
    "\n",
    "Doing this prevents our model from interpreting duplicate words as two separate features and reduces the number of dimensions our model has to process (increasing efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = lambda string : \" \".join(re.findall(pattern=pattern, string=string))\n",
    "\n",
    "data[\"Summary\"] = data[\"Summary\"].apply(tokenizer)\n",
    "data[\"Text\"] = data[\"Text\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Filler Words\n",
    "Some words like \"I\", \"the\", \"a\", etc. don't impact the sentiment of the text content. Remove these words from all review content so there is less redundant features for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "\n",
      "Sample of English stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "Removing stop words from Summary...\n",
      "Removing stop words from Text...\n",
      "\n",
      "Sample of processed Summary:\n",
      "0    good quality dog food\n",
      "1               advertised\n",
      "2             delight says\n",
      "3           cough medicine\n",
      "4              great taffy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Sample of processed Text:\n",
      "0    bought several vitality canned dog food produc...\n",
      "1    product arrived labeled jumbo salted peanuts p...\n",
      "2    confection around centuries light pillowy citr...\n",
      "3    looking secret ingredient robitussin believe f...\n",
      "4    great taffy great price wide assortment yummy ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print sample of stopwords\n",
    "print(\"\\nSample of English stopwords:\")\n",
    "print(sorted(list(stop_words))[:10])  # Print first 10 stopwords\n",
    "\n",
    "def remove_stopwords_from_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Process both columns\n",
    "print(\"Removing stop words from Summary...\")\n",
    "data['Summary'] = data['Summary'].apply(remove_stopwords_from_text)\n",
    "\n",
    "print(\"Removing stop words from Text...\")\n",
    "data['Text'] = data['Text'].apply(remove_stopwords_from_text)\n",
    "\n",
    "# Print samples for verification\n",
    "print(\"\\nSample of processed Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of processed Text:\")\n",
    "print(data['Text'].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Data Points\n",
    "The first stage of data cleaning is to identify and remove data points that aren't related to our task. In \"Amazon Fine Food Reviews\", we have many different product reviews including: pet food, medicine, microwavable food, fine foods, etc.\n",
    "- Is this category of food or type of review relevant to our task?\n",
    "- Would removing this type of review from the data improve the accuracy of our model?\n",
    "- If we remove this type of review, how will it effect our training process (would there be too little data remaining?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_aspects = {\n",
    "    \"pet_species\":[\n",
    "        \"dog\",\"cat\",\"puppy\",\"kitten\",\"fish\",\"hamster\",\"rabbit\",\"guinea pig\",\"bird\",\"parrot\",\"turtle\",\n",
    "        \"lizard\", \"snake\", \"ferret\", \"gerbil\", \"chinchilla\", \"mouse\", \"rat\", \"iguana\", \"gecko\",\n",
    "        \"dogs\",\"cats\",\"puppys\",\"kittens\",\"fishs\",\"hamsters\",\"rabbits\",\"guinea pigs\",\"birds\",\"parrots\",\"turtles\",\n",
    "        \"lizards\", \"snakes\", \"ferrets\", \"gerbils\", \"chinchillas\", \"mouses\", \"rats\", \"iguanas\", \"geckos\"\n",
    "    ],\n",
    "    \"pet_food_brands\":[\n",
    "        \"purina\", \"pedigre\", \"iams\", \"blue buffalo\", \"hill science diet\", \"royal canin\", \"fancy feast\", \"friskies\",\n",
    "        \"cesar\", \"meow mix\", \"nutro\", \"wellness\", \"orijen\", \"acana\", \"greenies\", \"temptations\", \"whiskas\"\n",
    "    ],\n",
    "    \"digestive\": [\n",
    "        \"nausea\", \"vomiting\", \"diarrhea\", \"constipation\", \"bloating\",\n",
    "        \"stomach ache\", \"indigestion\", \"heartburn\", \"cramps\", \"gas\"\n",
    "    ],\n",
    "    \"neurological\": [\n",
    "        \"headache\", \"migraine\", \"dizziness\", \"fatigue\", \"insomnia\",\n",
    "        \"brain fog\", \"numbness\", \"tingling\", \"vertigo\"\n",
    "    ],\n",
    "    \"respiratory\": [\n",
    "        \"cough\", \"shortness of breath\", \"wheezing\", \"congestion\",\n",
    "        \"runny nose\", \"sore throat\", \"sneezing\"\n",
    "    ],\n",
    "    \"skin\": [\n",
    "        \"rash\", \"itching\", \"hives\", \"acne\", \"eczema\", \"redness\",\n",
    "        \"dry skin\", \"swelling\"\n",
    "    ],\n",
    "    \"pain\": [\n",
    "        \"pain\", \"ache\", \"soreness\", \"stiffness\", \"joint pain\",\n",
    "        \"back pain\", \"chest pain\", \"muscle pain\"\n",
    "    ],\n",
    "    \"psychological\": [\n",
    "        \"anxiety\", \"depression\", \"irritability\", \"mood swings\",\n",
    "        \"panic attacks\", \"restlessness\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"fever\", \"chills\", \"sweating\", \"weakness\", \"loss of appetite\",\n",
    "        \"weight loss\", \"weight gain\"\n",
    "    ],\n",
    "    \"allergic\": [\n",
    "        \"allergy\", \"anaphylaxis\", \"sensitivity\", \"intolerance\",\n",
    "        \"swelling of the lips\", \"swelling of the throat\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_prod(value, dataframe, series):\n",
    "    products = set()\n",
    "\n",
    "    for i, string in enumerate(series):\n",
    "        if re.search(pattern=f\"{value} \", string=string):\n",
    "            products.add(dataframe.iloc[i][\"ProductId\"])\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in non_aspects.keys():\n",
    "    for value in non_aspects[key]:\n",
    "        sum_id = search_prod(value=value, dataframe=data, series=data[\"Summary\"])\n",
    "        txt_id = search_prod(value=value, dataframe=data, series=data[\"Text\"])\n",
    "        prod_id = sum_id.union(txt_id)\n",
    "        \n",
    "        data = data[~data[\"ProductId\"].isin(prod_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>203541.000000</td>\n",
       "      <td>203541.000000</td>\n",
       "      <td>203541.000000</td>\n",
       "      <td>203541.000000</td>\n",
       "      <td>2.035410e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>285417.564049</td>\n",
       "      <td>1.470637</td>\n",
       "      <td>1.880093</td>\n",
       "      <td>4.211058</td>\n",
       "      <td>1.296676e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164147.338309</td>\n",
       "      <td>4.104564</td>\n",
       "      <td>4.716213</td>\n",
       "      <td>1.312078</td>\n",
       "      <td>4.933690e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.617184e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>141584.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.270512e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>283207.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.313107e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>428271.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.334102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>593.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  203541.000000         203541.000000           203541.000000   \n",
       "mean   285417.564049              1.470637                1.880093   \n",
       "std    164147.338309              4.104564                4.716213   \n",
       "min         2.000000              0.000000                0.000000   \n",
       "25%    141584.000000              0.000000                0.000000   \n",
       "50%    283207.000000              0.000000                1.000000   \n",
       "75%    428271.000000              2.000000                2.000000   \n",
       "max    568454.000000            580.000000              593.000000   \n",
       "\n",
       "               Score          Time  \n",
       "count  203541.000000  2.035410e+05  \n",
       "mean        4.211058  1.296676e+09  \n",
       "std         1.312078  4.933690e+07  \n",
       "min         1.000000  9.617184e+08  \n",
       "25%         4.000000  1.270512e+09  \n",
       "50%         5.000000  1.313107e+09  \n",
       "75%         5.000000  1.334102e+09  \n",
       "max         5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>advertised</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>delight says</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>nice taffy</td>\n",
       "      <td>got wild hair taffy ordered five pound bag taf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>great good expensive brands</td>\n",
       "      <td>saltwater taffy great flavors soft chewy candy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "4                     0                       0      5  1350777600   \n",
       "5                     0                       0      4  1342051200   \n",
       "6                     0                       0      5  1340150400   \n",
       "\n",
       "                       Summary  \\\n",
       "1                   advertised   \n",
       "2                 delight says   \n",
       "4                  great taffy   \n",
       "5                   nice taffy   \n",
       "6  great good expensive brands   \n",
       "\n",
       "                                                Text  \n",
       "1  product arrived labeled jumbo salted peanuts p...  \n",
       "2  confection around centuries light pillowy citr...  \n",
       "4  great taffy great price wide assortment yummy ...  \n",
       "5  got wild hair taffy ordered five pound bag taf...  \n",
       "6  saltwater taffy great flavors soft chewy candy...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Uncecessary Columns\n",
    "- What columns are necessary for our model? \n",
    "- Is there anything that needs to be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include features that can be plotted in correlation matrix\n",
    "# String features cannot be intepreted in correlation matrix\n",
    "numeric_data = data.drop(columns=[\"ProductId\", \"UserId\", \"ProfileName\", \"Summary\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Text\"] = data[\"Summary\"].fillna(\"\") + \" \" + data[\"Text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen in the data exploration stage, most numerical features excluding \n",
    "# the newly created \"Helpfulness\" were not indicative of Score\n",
    "data = data.drop(columns=[\n",
    "    \"Id\",\n",
    "    \"UserId\", \n",
    "    \"ProfileName\", \n",
    "    \"HelpfulnessNumerator\", \n",
    "    \"HelpfulnessDenominator\",\n",
    "    \"Time\",\n",
    "    \"Summary\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>1</td>\n",
       "      <td>advertised product arrived labeled jumbo salte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>4</td>\n",
       "      <td>delight says confection around centuries light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>5</td>\n",
       "      <td>great taffy great taffy great price wide assor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>4</td>\n",
       "      <td>nice taffy got wild hair taffy ordered five po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>5</td>\n",
       "      <td>great good expensive brands saltwater taffy gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId  Score                                               Text\n",
       "1  B00813GRG4      1  advertised product arrived labeled jumbo salte...\n",
       "2  B000LQOCH0      4  delight says confection around centuries light...\n",
       "4  B006K2ZZ7K      5  great taffy great taffy great price wide assor...\n",
       "5  B006K2ZZ7K      4  nice taffy got wild hair taffy ordered five po...\n",
       "6  B006K2ZZ7K      5  great good expensive brands saltwater taffy gr..."
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Columns to Numerical\n",
    "For the more complex columns we will be doing word embedding. However, features such as 'ProductId' can be converted into numerical form so the model has an easier time interpreting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ProductId\"] = pd.factorize(data[\"ProductId\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>advertised product arrived labeled jumbo salte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>delight says confection around centuries light...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>great taffy great taffy great price wide assor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>nice taffy got wild hair taffy ordered five po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>great good expensive brands saltwater taffy gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductId  Score                                               Text\n",
       "1          0      1  advertised product arrived labeled jumbo salte...\n",
       "2          1      4  delight says confection around centuries light...\n",
       "4          2      5  great taffy great taffy great price wide assor...\n",
       "5          2      4  nice taffy got wild hair taffy ordered five po...\n",
       "6          2      5  great good expensive brands saltwater taffy gr..."
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Split\n",
    "In this section we need to split the dataset into single entity and multiple entity data points. This step is necessary because the framework for our model requires that single entity data points are handled by **model A** and multiple entity data points are handled by **model B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa.framework.checkpoint_class.checkpoint_template import CheckpointManager\n",
    "from pyabsa.framework.checkpoint_class.checkpoint_template import available_checkpoints\n",
    "from pyabsa import AspectTermExtraction as ATEPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 15:32:47] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-13 15:32:47] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-13 15:32:47] (2.4.1.post1) Downloading checkpoint:multilingual \n",
      "[2025-05-13 15:32:47] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-05-13 15:32:47] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "Checkpoint downloaded to: ./checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\n"
     ]
    }
   ],
   "source": [
    "checkpoint = CheckpointManager()\n",
    "checkpoint_path = checkpoint._get_remote_checkpoint(checkpoint=\"multilingual\", task_code=\"ATEPC\")\n",
    "print(\"Checkpoint downloaded to:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 15:32:58] (2.4.1.post1) Please specify the task code, e.g. from pyabsa import TaskCodeOption\n",
      "[2025-05-13 15:32:58] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-13 15:32:58] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-13 15:32:58] (2.4.1.post1) Downloading checkpoint:multilingual \n",
      "[2025-05-13 15:32:58] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-05-13 15:32:58] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) config: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.config\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) state_dict: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.state_dict\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) model: None\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\\fast_lcf_atepc.tokenizer\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) Set Model Device: cuda:0\n",
      "[2025-05-13 15:32:59] (2.4.1.post1) Device Name: NVIDIA GeForce GTX 1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n",
      "c:\\Users\\Cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "preparing ate inference dataloader: 100%|██████████| 203541/203541 [08:54<00:00, 380.71it/s]\n",
      "extracting aspect terms: 100%|██████████| 6361/6361 [1:18:56<00:00,  1.34it/s]\n",
      "preparing apc inference dataloader: 100%|██████████| 380239/380239 [16:50<00:00, 376.36it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 11883/11883 [2:13:57<00:00,  1.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 19:33:13] (2.4.1.post1) The results of aspect term extraction have been saved in d:\\Documents\\Education\\nn-fuzz-proj\\Amazon-Sentiment-Analysis\\src\\cleaning\\Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\n"
     ]
    }
   ],
   "source": [
    "# you can view all available checkpoints by calling available_checkpoints()\n",
    "checkpoint_map = available_checkpoints()\n",
    "\n",
    "aspect_extractor = ATEPC.AspectExtractor(\n",
    "    'multilingual',\n",
    "    auto_device=True,  # False means load model on CPU\n",
    "    cal_perplexity=True,\n",
    ")\n",
    "\n",
    "# instance inference\n",
    "text_aspects = aspect_extractor.batch_predict(\n",
    "    data[\"Text\"].tolist(),\n",
    "    save_result=True,\n",
    "    pred_sentiment=True,  # Predict the sentiment of extracted aspect terms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(RESULTS_PATH, 'r') as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_to_score = {\n",
    "    \"Negative\" : 1,\n",
    "    \"Neutral\"  : 2,\n",
    "    \"Positive\" : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of all unique aspect terms\n",
    "all_aspects = set()\n",
    "\n",
    "for result in results:\n",
    "    for aspect in result.get(\"aspect\", []):\n",
    "        all_aspects.add(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each aspect a column index\n",
    "aspect_names = sorted(all_aspects)\n",
    "aspect_vocab = {aspect: idx for idx, aspect in enumerate(aspect_names)}\n",
    "n_samples    = len(results)\n",
    "n_features   = len(aspect_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrix using LIL (good for row-wise construction)\n",
    "sentiment_matrix = lil_matrix((n_samples, n_features), dtype=np.float32)\n",
    "\n",
    "# Keep track of rows with one or fewer aspect counts\n",
    "nonzero_counts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the matrix\n",
    "for i, result in enumerate(results):\n",
    "    aspects     = result.get(\"aspect\", [])\n",
    "    sentiments  = result.get(\"sentiment\", [])\n",
    "    count       = 0\n",
    "    for aspect, sentiment in zip(aspects, sentiments):\n",
    "        col_idx = aspect_vocab.get(aspect)\n",
    "        if col_idx is not None:\n",
    "            sentiment_score = sentiment_to_score.get(sentiment, 0)\n",
    "            sentiment_matrix[i, col_idx] = sentiment_score\n",
    "            count += 1\n",
    "\n",
    "    nonzero_counts.append(count)\n",
    "\n",
    "# Convert to CSR for efficient arithmetic / storage\n",
    "sentiment_matrix = sentiment_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter indicies\n",
    "low_aspect_indicies     = [i for i, count in enumerate(nonzero_counts) if count <= 1]\n",
    "high_aspect_indicies    = [i for i, count in enumerate(nonzero_counts) if count > 1]\n",
    "\n",
    "# Create two separate matrices\n",
    "low_aspect_matrix   = sentiment_matrix[low_aspect_indicies]\n",
    "high_aspect_matrix  = sentiment_matrix[high_aspect_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's say:\n",
    "    - \"sentiment_matrix\" is your sparse matrix (CSR format)\n",
    "    - \"data\" is your original DataFrame with columns like \"Score\", \"ProductID\", \"Helpfulness\"\n",
    "'''\n",
    "\n",
    "# Dense columns for inclusion into sparse matrix\n",
    "dense_features = [\"ProductId\", \"Helpfulness\", \"Score\"]\n",
    "\n",
    "# Get dense data for each split\n",
    "low_dense   = data.iloc[low_aspect_indicies][dense_features].values\n",
    "high_dense  = data.iloc[high_aspect_indicies][dense_features].values\n",
    "\n",
    "# Convert dense to sparse and stack\n",
    "low_combined    = hstack([csr_matrix(low_dense), low_aspect_matrix])\n",
    "high_combined   = hstack([csr_matrix(high_dense), high_aspect_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dense_features + aspect_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to External Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi_aspect.pkl']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both matrix and column names\n",
    "joblib.dump({\n",
    "    \"matrix\": low_combined,\n",
    "    \"columns\": column_names\n",
    "}, \"single_aspect.pkl\")\n",
    "\n",
    "joblib.dump({\n",
    "    \"matrix\": high_combined,\n",
    "    \"columns\": column_names\n",
    "}, \"multi_aspect.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
