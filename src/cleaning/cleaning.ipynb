{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import required packages and libraries for data exploration\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up file path and data handling objects\n",
    "'''\n",
    "PATH = \"../data/reviews.csv\"\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.00000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>5.684540e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>1.743817</td>\n",
       "      <td>2.22881</td>\n",
       "      <td>4.183199</td>\n",
       "      <td>1.296257e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164098.679298</td>\n",
       "      <td>7.636513</td>\n",
       "      <td>8.28974</td>\n",
       "      <td>1.310436</td>\n",
       "      <td>4.804331e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142114.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.271290e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>284227.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.311120e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>426340.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332720e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>923.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
       "count  568454.000000         568454.000000            568454.00000   \n",
       "mean   284227.500000              1.743817                 2.22881   \n",
       "std    164098.679298              7.636513                 8.28974   \n",
       "min         1.000000              0.000000                 0.00000   \n",
       "25%    142114.250000              0.000000                 0.00000   \n",
       "50%    284227.500000              0.000000                 1.00000   \n",
       "75%    426340.750000              2.000000                 2.00000   \n",
       "max    568454.000000            866.000000               923.00000   \n",
       "\n",
       "               Score          Time  \n",
       "count  568454.000000  5.684540e+05  \n",
       "mean        4.183199  1.296257e+09  \n",
       "std         1.310436  4.804331e+07  \n",
       "min         1.000000  9.393408e+08  \n",
       "25%         4.000000  1.271290e+09  \n",
       "50%         5.000000  1.311120e+09  \n",
       "75%         5.000000  1.332720e+09  \n",
       "max         5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Sensitivity\n",
    "Convert the input features in the raw dataset into a case insensitive format (all lowercase/uppercase) to reduce the amount of distinct words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values from tokenizer strings\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\")\n",
    "data[\"Text\"] = data[\"Text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"delight\" says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy at a great price.  there was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  good quality dog food  i have bought several of the vitality canned d...  \n",
       "1      not as advertised  product arrived labeled as jumbo salted peanut...  \n",
       "2  \"delight\" says it all  this is a confection that has been around a fe...  \n",
       "3         cough medicine  if you are looking for the secret ingredient i...  \n",
       "4            great taffy  great taffy at a great price.  there was a wid...  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all words to lowercase to reduce the number of unique features\n",
    "data[\"Summary\"] = data[\"Summary\"].str.lower()\n",
    "data[\"Text\"] = data[\"Text\"].str.lower()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Handling\n",
    "Some words that contain punctuation can be recorded as separate features without punctuation handling (e.g., \"Steve's pizza is great!\" and \"Steve makes great pizza!\").\n",
    "\n",
    "| is | great | great! | makes | pizza | pizza! | Steve | Steve's |\n",
    "|----|-------|--------|-------|-------|--------|-------|---------|\n",
    "|1   | 1     | 1      | 1     | 1     | 1      | 1     | 1       |\n",
    "\n",
    "We want to remove uncessesary punctuation so that we don't have duplicates of effectively the same word.\n",
    "| is | great | makes | pizza | Steve |\n",
    "|----|-------|-------|-------|-------|\n",
    "| 1  | 2     | 1     | 2     | 2     |\n",
    "\n",
    "Doing this prevents our model from interpreting duplicate words as two separate features and reduces the number of dimensions our model has to process (increasing efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokenizer = lambda string : \" \".join(re.findall(pattern=pattern, string=string))\n",
    "\n",
    "data[\"Summary\"] = data[\"Summary\"].apply(tokenizer)\n",
    "data[\"Text\"] = data[\"Text\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Filler Words\n",
    "Some words like \"I\", \"the\", \"a\", etc. don't impact the sentiment of the text content. Remove these words from all review content so there is less redundant features for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "\n",
      "Sample of English stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
      "Removing stop words from Summary...\n",
      "Removing stop words from Text...\n",
      "\n",
      "Sample of processed Summary:\n",
      "0    good quality dog food\n",
      "1               advertised\n",
      "2             delight says\n",
      "3           cough medicine\n",
      "4              great taffy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Sample of processed Text:\n",
      "0    bought several vitality canned dog food produc...\n",
      "1    product arrived labeled jumbo salted peanuts p...\n",
      "2    confection around centuries light pillowy citr...\n",
      "3    looking secret ingredient robitussin believe f...\n",
      "4    great taffy great price wide assortment yummy ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords    \n",
    "\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print sample of stopwords\n",
    "print(\"\\nSample of English stopwords:\")\n",
    "print(sorted(list(stop_words))[:10])  # Print first 10 stopwords\n",
    "\n",
    "def remove_stopwords_from_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = text.lower().split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Process both columns\n",
    "print(\"Removing stop words from Summary...\")\n",
    "data['Summary'] = data['Summary'].apply(remove_stopwords_from_text)\n",
    "\n",
    "print(\"Removing stop words from Text...\")\n",
    "data['Text'] = data['Text'].apply(remove_stopwords_from_text)\n",
    "\n",
    "# Print samples for verification\n",
    "print(\"\\nSample of processed Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of processed Text:\")\n",
    "print(data['Text'].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    result = \" \".join([token.lemma_ for token in doc])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process both columns\n",
    "print(\"Lemmatizing Summary...\")\n",
    "data['Summary'] = data['Summary'].apply(lemmatize)\n",
    "\n",
    "print(\"Lemmatizing Text...\")\n",
    "data['Text'] = data['Text'].apply(lemmatize)\n",
    "\n",
    "# Print samples for verification\n",
    "print(\"\\nSample of processed Summary:\")\n",
    "print(data['Summary'].head())\n",
    "print(\"\\nSample of processed Text:\")\n",
    "print(data['Text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/lemmatize.csv\")\n",
    "data = pd.read_csv(\"../data/lemmatize.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/lemmatize.csv\")\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\")\n",
    "data[\"Text\"] = data[\"Text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Data Points\n",
    "The first stage of data cleaning is to identify and remove data points that aren't related to our task. In \"Amazon Fine Food Reviews\", we have many different product reviews including: pet food, medicine, microwavable food, fine foods, etc.\n",
    "- Is this category of food or type of review relevant to our task?\n",
    "- Would removing this type of review from the data improve the accuracy of our model?\n",
    "- If we remove this type of review, how will it effect our training process (would there be too little data remaining?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_aspects = {\n",
    "    \"pet_species\":[\n",
    "        \"dog\",\"cat\",\"puppy\",\"kitten\",\"fish\",\"hamster\",\"rabbit\",\"guinea pig\",\"bird\",\"parrot\",\"turtle\",\n",
    "        \"lizard\", \"snake\", \"ferret\", \"gerbil\", \"chinchilla\", \"mouse\", \"rat\", \"iguana\", \"gecko\",\n",
    "        \"dogs\",\"cats\",\"puppys\",\"kittens\",\"fishs\",\"hamsters\",\"rabbits\",\"guinea pigs\",\"birds\",\"parrots\",\"turtles\",\n",
    "        \"lizards\", \"snakes\", \"ferrets\", \"gerbils\", \"chinchillas\", \"mouses\", \"rats\", \"iguanas\", \"geckos\"\n",
    "    ],\n",
    "    \"pet_food_brands\":[\n",
    "        \"purina\", \"pedigre\", \"iams\", \"blue buffalo\", \"hill science diet\", \"royal canin\", \"fancy feast\", \"friskies\",\n",
    "        \"cesar\", \"meow mix\", \"nutro\", \"wellness\", \"orijen\", \"acana\", \"greenies\", \"temptations\", \"whiskas\"\n",
    "    ],\n",
    "    \"digestive\": [\n",
    "        \"nausea\", \"vomiting\", \"diarrhea\", \"constipation\", \"bloating\",\n",
    "        \"stomach ache\", \"indigestion\", \"heartburn\", \"cramps\", \"gas\"\n",
    "    ],\n",
    "    \"neurological\": [\n",
    "        \"headache\", \"migraine\", \"dizziness\", \"fatigue\", \"insomnia\",\n",
    "        \"brain fog\", \"numbness\", \"tingling\", \"vertigo\"\n",
    "    ],\n",
    "    \"respiratory\": [\n",
    "        \"cough\", \"shortness of breath\", \"wheezing\", \"congestion\",\n",
    "        \"runny nose\", \"sore throat\", \"sneezing\"\n",
    "    ],\n",
    "    \"skin\": [\n",
    "        \"rash\", \"itching\", \"hives\", \"acne\", \"eczema\", \"redness\",\n",
    "        \"dry skin\", \"swelling\"\n",
    "    ],\n",
    "    \"pain\": [\n",
    "        \"pain\", \"ache\", \"soreness\", \"stiffness\", \"joint pain\",\n",
    "        \"back pain\", \"chest pain\", \"muscle pain\"\n",
    "    ],\n",
    "    \"psychological\": [\n",
    "        \"anxiety\", \"depression\", \"irritability\", \"mood swings\",\n",
    "        \"panic attacks\", \"restlessness\"\n",
    "    ],\n",
    "    \"general\": [\n",
    "        \"fever\", \"chills\", \"sweating\", \"weakness\", \"loss of appetite\",\n",
    "        \"weight loss\", \"weight gain\"\n",
    "    ],\n",
    "    \"allergic\": [\n",
    "        \"allergy\", \"anaphylaxis\", \"sensitivity\", \"intolerance\",\n",
    "        \"swelling of the lips\", \"swelling of the throat\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>568454.00000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>5.684540e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>284226.500000</td>\n",
       "      <td>284227.500000</td>\n",
       "      <td>1.743817</td>\n",
       "      <td>2.22881</td>\n",
       "      <td>4.183199</td>\n",
       "      <td>1.296257e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164098.679298</td>\n",
       "      <td>164098.679298</td>\n",
       "      <td>7.636513</td>\n",
       "      <td>8.28974</td>\n",
       "      <td>1.310436</td>\n",
       "      <td>4.804331e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.393408e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142113.250000</td>\n",
       "      <td>142114.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.271290e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>284226.500000</td>\n",
       "      <td>284227.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.311120e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>426339.750000</td>\n",
       "      <td>426340.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.332720e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568453.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>866.000000</td>\n",
       "      <td>923.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0             Id  HelpfulnessNumerator  \\\n",
       "count  568454.000000  568454.000000         568454.000000   \n",
       "mean   284226.500000  284227.500000              1.743817   \n",
       "std    164098.679298  164098.679298              7.636513   \n",
       "min         0.000000       1.000000              0.000000   \n",
       "25%    142113.250000  142114.250000              0.000000   \n",
       "50%    284226.500000  284227.500000              0.000000   \n",
       "75%    426339.750000  426340.750000              2.000000   \n",
       "max    568453.000000  568454.000000            866.000000   \n",
       "\n",
       "       HelpfulnessDenominator          Score          Time  \n",
       "count            568454.00000  568454.000000  5.684540e+05  \n",
       "mean                  2.22881       4.183199  1.296257e+09  \n",
       "std                   8.28974       1.310436  4.804331e+07  \n",
       "min                   0.00000       1.000000  9.393408e+08  \n",
       "25%                   0.00000       4.000000  1.271290e+09  \n",
       "50%                   1.00000       5.000000  1.311120e+09  \n",
       "75%                   2.00000       5.000000  1.332720e+09  \n",
       "max                 923.00000       5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>buy several vitality can dog food product find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>advertise</td>\n",
       "      <td>product arrive label jumbo salt peanut peanut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>delight say</td>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>look secret ingredient robitussin believe find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id   ProductId          UserId  \\\n",
       "0           0   1  B001E4KFG0  A3SGXH7AUHU8GW   \n",
       "1           1   2  B00813GRG4  A1D87F6ZCVE5NK   \n",
       "2           2   3  B000LQOCH0   ABXLMWJIXXAIN   \n",
       "3           3   4  B000UA0QIQ  A395BORC6FGVXV   \n",
       "4           4   5  B006K2ZZ7K  A1UQRSCLF8GW1T   \n",
       "\n",
       "                       ProfileName  HelpfulnessNumerator  \\\n",
       "0                       delmartian                     1   \n",
       "1                           dll pa                     0   \n",
       "2  Natalia Corres \"Natalia Corres\"                     1   \n",
       "3                             Karl                     3   \n",
       "4    Michael D. Bigham \"M. Wassir\"                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  good quality dog food   \n",
       "1                       0      1  1346976000              advertise   \n",
       "2                       1      4  1219017600            delight say   \n",
       "3                       3      2  1307923200         cough medicine   \n",
       "4                       0      5  1350777600            great taffy   \n",
       "\n",
       "                                                Text  \n",
       "0  buy several vitality can dog food product find...  \n",
       "1  product arrive label jumbo salt peanut peanut ...  \n",
       "2  confection around century light pillowy citrus...  \n",
       "3  look secret ingredient robitussin believe find...  \n",
       "4  great taffy great price wide assortment yummy ...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_prod(value, dataframe, series):\n",
    "    products = set()\n",
    "\n",
    "    for i, string in enumerate(series):\n",
    "        for word in string.split():\n",
    "            if value == word:\n",
    "                products.add(dataframe.iloc[i][\"ProductId\"])\n",
    "                break\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in non_aspects.keys():\n",
    "    for value in non_aspects[key]:\n",
    "        sum_id = search_prod(value=value, dataframe=data, series=data[\"Summary\"])\n",
    "        txt_id = search_prod(value=value, dataframe=data, series=data[\"Text\"])\n",
    "        prod_id = sum_id.union(txt_id)\n",
    "\n",
    "        data = data[~data[\"ProductId\"].isin(prod_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>213513.000000</td>\n",
       "      <td>213513.000000</td>\n",
       "      <td>213513.000000</td>\n",
       "      <td>213513.000000</td>\n",
       "      <td>213513.000000</td>\n",
       "      <td>2.135130e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>286390.488851</td>\n",
       "      <td>286391.488851</td>\n",
       "      <td>1.555643</td>\n",
       "      <td>1.978381</td>\n",
       "      <td>4.201880</td>\n",
       "      <td>1.296387e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164426.745625</td>\n",
       "      <td>164426.745625</td>\n",
       "      <td>5.470705</td>\n",
       "      <td>6.003509</td>\n",
       "      <td>1.319164</td>\n",
       "      <td>4.957523e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.617184e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>142760.000000</td>\n",
       "      <td>142761.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.270253e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>285247.000000</td>\n",
       "      <td>285248.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.312848e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>429994.000000</td>\n",
       "      <td>429995.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.333930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>568453.000000</td>\n",
       "      <td>568454.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>544.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.351210e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0             Id  HelpfulnessNumerator  \\\n",
       "count  213513.000000  213513.000000         213513.000000   \n",
       "mean   286390.488851  286391.488851              1.555643   \n",
       "std    164426.745625  164426.745625              5.470705   \n",
       "min         1.000000       2.000000              0.000000   \n",
       "25%    142760.000000  142761.000000              0.000000   \n",
       "50%    285247.000000  285248.000000              0.000000   \n",
       "75%    429994.000000  429995.000000              2.000000   \n",
       "max    568453.000000  568454.000000            538.000000   \n",
       "\n",
       "       HelpfulnessDenominator          Score          Time  \n",
       "count           213513.000000  213513.000000  2.135130e+05  \n",
       "mean                 1.978381       4.201880  1.296387e+09  \n",
       "std                  6.003509       1.319164  4.957523e+07  \n",
       "min                  0.000000       1.000000  9.617184e+08  \n",
       "25%                  0.000000       4.000000  1.270253e+09  \n",
       "50%                  1.000000       5.000000  1.312848e+09  \n",
       "75%                  2.000000       5.000000  1.333930e+09  \n",
       "max                544.000000       5.000000  1.351210e+09  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>advertise</td>\n",
       "      <td>product arrive label jumbo salt peanut peanut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>delight say</td>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>nice taffy</td>\n",
       "      <td>get wild hair taffy order five pound bag taffy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>great good expensive brand</td>\n",
       "      <td>saltwater taffy great flavor soft chewy candy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id   ProductId          UserId  \\\n",
       "1           1   2  B00813GRG4  A1D87F6ZCVE5NK   \n",
       "2           2   3  B000LQOCH0   ABXLMWJIXXAIN   \n",
       "4           4   5  B006K2ZZ7K  A1UQRSCLF8GW1T   \n",
       "5           5   6  B006K2ZZ7K   ADT0SRK1MGOEU   \n",
       "6           6   7  B006K2ZZ7K  A1SP2KVKFXXRU1   \n",
       "\n",
       "                       ProfileName  HelpfulnessNumerator  \\\n",
       "1                           dll pa                     0   \n",
       "2  Natalia Corres \"Natalia Corres\"                     1   \n",
       "4    Michael D. Bigham \"M. Wassir\"                     0   \n",
       "5                   Twoapennything                     0   \n",
       "6                David C. Sullivan                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                     Summary  \\\n",
       "1                       0      1  1346976000                   advertise   \n",
       "2                       1      4  1219017600                 delight say   \n",
       "4                       0      5  1350777600                 great taffy   \n",
       "5                       0      4  1342051200                  nice taffy   \n",
       "6                       0      5  1340150400  great good expensive brand   \n",
       "\n",
       "                                                Text  \n",
       "1  product arrive label jumbo salt peanut peanut ...  \n",
       "2  confection around century light pillowy citrus...  \n",
       "4  great taffy great price wide assortment yummy ...  \n",
       "5  get wild hair taffy order five pound bag taffy...  \n",
       "6  saltwater taffy great flavor soft chewy candy ...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Uncecessary Columns\n",
    "- What columns are necessary for our model? \n",
    "- Is there anything that needs to be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Text\"] = data[\"Summary\"].fillna(\"\") + \" \" + data[\"Text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen in the data exploration stage, most numerical features excluding \n",
    "# the newly created \"Helpfulness\" were not indicative of Score\n",
    "data = data.drop(columns=[\n",
    "    \"Id\",\n",
    "    \"UserId\", \n",
    "    \"ProfileName\", \n",
    "    \"HelpfulnessNumerator\", \n",
    "    \"HelpfulnessDenominator\",\n",
    "    \"Time\",\n",
    "    \"Summary\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>1</td>\n",
       "      <td>advertise product arrive label jumbo salt pean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>4</td>\n",
       "      <td>delight say confection around century light pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>5</td>\n",
       "      <td>great taffy great taffy great price wide assor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>4</td>\n",
       "      <td>nice taffy get wild hair taffy order five poun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>5</td>\n",
       "      <td>great good expensive brand saltwater taffy gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   ProductId  Score  \\\n",
       "1           1  B00813GRG4      1   \n",
       "2           2  B000LQOCH0      4   \n",
       "4           4  B006K2ZZ7K      5   \n",
       "5           5  B006K2ZZ7K      4   \n",
       "6           6  B006K2ZZ7K      5   \n",
       "\n",
       "                                                Text  \n",
       "1  advertise product arrive label jumbo salt pean...  \n",
       "2  delight say confection around century light pi...  \n",
       "4  great taffy great taffy great price wide assor...  \n",
       "5  nice taffy get wild hair taffy order five poun...  \n",
       "6  great good expensive brand saltwater taffy gre...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Columns to Numerical\n",
    "For the more complex columns we will be doing word embedding. However, features such as 'ProductId' can be converted into numerical form so the model has an easier time interpreting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ProductId to numerical values for modelling input\n",
    "data[\"ProductId\"] = pd.factorize(data[\"ProductId\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>advertise product arrive label jumbo salt pean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>delight say confection around century light pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>great taffy great taffy great price wide assor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>nice taffy get wild hair taffy order five poun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>great good expensive brand saltwater taffy gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ProductId  Score  \\\n",
       "1           1          0      1   \n",
       "2           2          1      4   \n",
       "4           4          2      5   \n",
       "5           5          2      4   \n",
       "6           6          2      5   \n",
       "\n",
       "                                                Text  \n",
       "1  advertise product arrive label jumbo salt pean...  \n",
       "2  delight say confection around century light pi...  \n",
       "4  great taffy great taffy great price wide assor...  \n",
       "5  nice taffy get wild hair taffy order five poun...  \n",
       "6  great good expensive brand saltwater taffy gre...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213513"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Split\n",
    "In this section we need to split the dataset into single entity and multiple entity data points. This step is necessary because the framework for our model requires that single entity data points are handled by **model A** and multiple entity data points are handled by **model B**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-31 21:18:59] (2.4.1.post1) PyABSA(2.4.1.post1): If your code crashes on Colab, please use the GPU runtime. Then run \"pip install pyabsa[dev] -U\" and restart the kernel.\n",
      "Or if it does not work, you can use v1.x versions, e.g., pip install pyabsa<2.0 -U\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WARNING: When you fails to load a checkpoint, e.g., Unexpected key(s),\n",
      "Try to downgrade transformers<=4.29.0.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyrus\\anaconda3\\envs\\tfenv\\lib\\multiprocessing\\pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "  _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import pyabsa\n",
    "from pyabsa.framework.checkpoint_class.checkpoint_template import CheckpointManager\n",
    "from pyabsa.framework.checkpoint_class.checkpoint_template import available_checkpoints\n",
    "from pyabsa import AspectTermExtraction as ATEPC\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-31 21:19:16] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Downloading checkpoint:multilingual \n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "Checkpoint downloaded to: ./checkpoints\\ATEPC_MULTILINGUAL_CHECKPOINT\n"
     ]
    }
   ],
   "source": [
    "checkpoint = CheckpointManager()\n",
    "checkpoint_path = checkpoint._get_remote_checkpoint(checkpoint=\"multilingual\", task_code=\"ATEPC\")\n",
    "print(\"Checkpoint downloaded to:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-31 21:19:16] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) ********** Available ATEPC model checkpoints for Version:2.4.1.post1 (this version) **********\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Downloading checkpoint:english \n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Notice: The pretrained model are used for testing, it is recommended to train the model on your own custom datasets\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Checkpoint already downloaded, skip\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) Load aspect extractor from checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) config: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.config\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) state_dict: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.state_dict\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) model: None\n",
      "[2025-05-31 21:19:16] (2.4.1.post1) tokenizer: checkpoints\\ATEPC_ENGLISH_CHECKPOINT\\fast_lcf_atepc_English_cdw_apcacc_82.36_apcf1_81.89_atef1_75.43\\fast_lcf_atepc.tokenizer\n",
      "[2025-05-31 21:19:17] (2.4.1.post1) Set Model Device: cuda:0\n",
      "[2025-05-31 21:19:17] (2.4.1.post1) Device Name: NVIDIA GeForce GTX 1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n",
      "c:\\Users\\Cyrus\\anaconda3\\envs\\tfenv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "aspect_extractor = ATEPC.AspectExtractor(\n",
    "    'english',\n",
    "    auto_device=True,  # False means load model on CPU\n",
    "    cal_perplexity=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:11<00:00, 446.67it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [04:11<00:00,  1.60s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8924/8924 [00:27<00:00, 323.05it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 279/279 [07:26<00:00,  1.60s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:10<00:00, 490.56it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8924/8924 [00:28<00:00, 313.39it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 279/279 [07:06<00:00,  1.53s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 524.30it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:48<00:00,  1.46s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8731/8731 [00:26<00:00, 332.71it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [07:00<00:00,  1.54s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:11<00:00, 450.78it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [04:02<00:00,  1.54s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8752/8752 [00:26<00:00, 324.47it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 274/274 [07:29<00:00,  1.64s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 509.28it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [04:16<00:00,  1.64s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8577/8577 [00:27<00:00, 316.17it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 269/269 [07:13<00:00,  1.61s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:10<00:00, 474.04it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [04:06<00:00,  1.57s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8928/8928 [00:27<00:00, 326.70it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 279/279 [07:18<00:00,  1.57s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 500.08it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:58<00:00,  1.52s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8829/8829 [00:27<00:00, 315.46it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 276/276 [06:56<00:00,  1.51s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:10<00:00, 482.59it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:43<00:00,  1.42s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8754/8754 [00:26<00:00, 331.34it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 274/274 [06:36<00:00,  1.45s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 538.25it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:39<00:00,  1.40s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8626/8626 [00:24<00:00, 347.12it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 270/270 [06:34<00:00,  1.46s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 537.48it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:43<00:00,  1.42s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8805/8805 [00:26<00:00, 337.00it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 276/276 [06:45<00:00,  1.47s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 532.71it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:44<00:00,  1.43s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8741/8741 [00:25<00:00, 341.15it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 274/274 [06:45<00:00,  1.48s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 526.77it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:44<00:00,  1.43s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8934/8934 [00:26<00:00, 342.26it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 280/280 [07:07<00:00,  1.53s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 529.49it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:54<00:00,  1.49s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8687/8687 [00:27<00:00, 312.04it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 272/272 [07:12<00:00,  1.59s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 516.12it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:55<00:00,  1.50s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8698/8698 [00:27<00:00, 318.68it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 272/272 [07:00<00:00,  1.55s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 523.66it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:58<00:00,  1.52s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8729/8729 [00:29<00:00, 295.21it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [06:54<00:00,  1.52s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:13<00:00, 373.04it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:43<00:00,  1.42s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8864/8864 [00:25<00:00, 344.77it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 277/277 [06:46<00:00,  1.47s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 538.06it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:40<00:00,  1.40s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8898/8898 [00:25<00:00, 354.73it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 279/279 [06:42<00:00,  1.44s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 513.68it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:39<00:00,  1.40s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8774/8774 [00:26<00:00, 335.13it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 275/275 [06:36<00:00,  1.44s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 524.35it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:32<00:00,  1.35s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8663/8663 [00:24<00:00, 357.09it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 271/271 [06:18<00:00,  1.40s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 526.77it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:31<00:00,  1.35s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8898/8898 [00:27<00:00, 318.39it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 279/279 [06:29<00:00,  1.39s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 537.45it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:31<00:00,  1.35s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8649/8649 [00:24<00:00, 350.11it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 271/271 [06:19<00:00,  1.40s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 536.99it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.38s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 9002/9002 [00:26<00:00, 341.85it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 282/282 [06:43<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 552.82it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8593/8593 [00:24<00:00, 355.73it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 269/269 [06:25<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 516.21it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8882/8882 [00:26<00:00, 334.40it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 278/278 [06:39<00:00,  1.44s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 510.21it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8625/8625 [00:24<00:00, 346.63it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 270/270 [06:26<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 531.05it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8736/8736 [00:25<00:00, 348.51it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [06:31<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 534.86it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8825/8825 [00:26<00:00, 336.77it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 276/276 [06:37<00:00,  1.44s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 533.29it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:33<00:00,  1.36s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8712/8712 [00:25<00:00, 339.78it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [06:22<00:00,  1.40s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 538.22it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:33<00:00,  1.36s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8662/8662 [00:23<00:00, 362.95it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 271/271 [06:21<00:00,  1.41s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:11<00:00, 433.85it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:36<00:00,  1.38s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8735/8735 [00:25<00:00, 344.08it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [06:31<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 530.15it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.39s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8669/8669 [00:25<00:00, 345.78it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 271/271 [06:28<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 526.14it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.38s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8792/8792 [00:25<00:00, 340.51it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 275/275 [06:34<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 531.58it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:33<00:00,  1.36s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8858/8858 [00:25<00:00, 344.58it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 277/277 [06:27<00:00,  1.40s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 543.94it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:32<00:00,  1.35s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8721/8721 [00:24<00:00, 356.93it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 273/273 [06:27<00:00,  1.42s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:11<00:00, 426.45it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:40<00:00,  1.41s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8643/8643 [00:25<00:00, 338.29it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 271/271 [06:32<00:00,  1.45s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 546.91it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:40<00:00,  1.41s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8548/8548 [00:24<00:00, 346.46it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 268/268 [06:22<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 528.01it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:37<00:00,  1.38s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8774/8774 [00:25<00:00, 350.46it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 275/275 [06:31<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 508.78it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:34<00:00,  1.37s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8681/8681 [00:24<00:00, 355.72it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 272/272 [06:28<00:00,  1.43s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 5000/5000 [00:09<00:00, 535.92it/s]\n",
      "extracting aspect terms: 100%|██████████| 157/157 [03:36<00:00,  1.38s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 8831/8831 [00:25<00:00, 345.11it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 276/276 [06:41<00:00,  1.45s/it]\n",
      "preparing ate inference dataloader: 100%|██████████| 3513/3513 [00:06<00:00, 554.16it/s]\n",
      "extracting aspect terms: 100%|██████████| 110/110 [02:35<00:00,  1.41s/it]\n",
      "preparing apc inference dataloader: 100%|██████████| 5933/5933 [00:18<00:00, 318.73it/s]\n",
      "classifying aspect sentiments: 100%|██████████| 186/186 [04:29<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 5000\n",
    "results     = []\n",
    "\n",
    "for i in range(15000, len(data), batch_size):\n",
    "    batch   = data[\"Text\"][i:i+batch_size].to_list()\n",
    "    res     = aspect_extractor.batch_predict(\n",
    "        target_file=batch,\n",
    "        save_result=False,\n",
    "        print_result=False,\n",
    "        pred_sentiment=True\n",
    "    )\n",
    "    results.extend(res)\n",
    "\n",
    "    if res is not None:\n",
    "        results.extend(res)\n",
    "\n",
    "        with open(f\"./absa_batch/results_batch{i}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(f\"Warning: batch starting at index {i} returned None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Aspect Term Extraction and Polarity Classification.FAST_LCF_ATEPC.result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_to_score = {\n",
    "    \"Negative\" : 1,\n",
    "    \"Neutral\"  : 2,\n",
    "    \"Positive\" : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH    = Path(\"./absa_batch\")\n",
    "results         = []\n",
    "\n",
    "# Open and load the JSON file\n",
    "for json_file in RESULTS_PATH.glob(\"*.json\"):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = json.load(f)\n",
    "        results.extend(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of all unique aspect terms\n",
    "all_aspects = set()\n",
    "\n",
    "for result in results:\n",
    "    for aspect in result.get(\"aspect\", []):\n",
    "        all_aspects.add(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35513"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each aspect a column index\n",
    "aspect_names = sorted(all_aspects)\n",
    "aspect_vocab = {aspect: idx for idx, aspect in enumerate(aspect_names)}\n",
    "n_samples    = len(results)\n",
    "n_features   = len(aspect_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrix using LIL (good for row-wise construction)\n",
    "sentiment_matrix = lil_matrix((n_samples, n_features), dtype=np.float32)\n",
    "\n",
    "# Keep track of rows with one or fewer aspect counts\n",
    "nonzero_counts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the matrix\n",
    "for i, result in enumerate(results):\n",
    "    aspects     = result.get(\"aspect\", [])\n",
    "    sentiments  = result.get(\"sentiment\", [])\n",
    "    count       = 0\n",
    "    for aspect, sentiment in zip(aspects, sentiments):\n",
    "        col_idx = aspect_vocab.get(aspect)\n",
    "        if col_idx is not None:\n",
    "            sentiment_score = sentiment_to_score.get(sentiment, 0)\n",
    "            sentiment_matrix[i, col_idx] = sentiment_score\n",
    "            count += 1\n",
    "\n",
    "    nonzero_counts.append(count)\n",
    "\n",
    "# Convert to CSR for efficient arithmetic / storage\n",
    "sentiment_matrix = sentiment_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter indicies\n",
    "low_aspect_indicies     = [i for i, count in enumerate(nonzero_counts) if count <= 1]\n",
    "high_aspect_indicies    = [i for i, count in enumerate(nonzero_counts) if count > 1]\n",
    "\n",
    "# Create two separate matrices\n",
    "low_aspect_matrix   = sentiment_matrix[low_aspect_indicies]\n",
    "high_aspect_matrix  = sentiment_matrix[high_aspect_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For single aspect and multi aspect split, let's save a copy of the raw copy to compare\n",
    "'''\n",
    "single_data     = data.iloc[low_aspect_indicies]\n",
    "multiple_data   = data.iloc[high_aspect_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Aspect Size: 104247\n",
      "Multiple Aspect Size: 109266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Single Aspect Size: {len(single_data)}\")\n",
    "print(f\"Multiple Aspect Size: {len(multiple_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104247"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_data.to_csv(\"./../data/single_raw.csv\")\n",
    "multiple_data.to_csv(\"./../data/multiple_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's say:\n",
    "    - \"sentiment_matrix\" is your sparse matrix (CSR format)\n",
    "    - \"data\" is your original DataFrame with columns like \"Score\", \"ProductID\", \"Helpfulness\"\n",
    "'''\n",
    "\n",
    "# Dense columns for inclusion into sparse matrix\n",
    "dense_features = [\"ProductId\", \"Score\"]\n",
    "\n",
    "# Get dense data for each split\n",
    "low_dense   = data.iloc[low_aspect_indicies][dense_features].values\n",
    "high_dense  = data.iloc[high_aspect_indicies][dense_features].values\n",
    "\n",
    "# Convert dense to sparse and stack\n",
    "low_combined    = hstack([csr_matrix(low_dense), low_aspect_matrix])\n",
    "high_combined   = hstack([csr_matrix(high_dense), high_aspect_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dense_features + aspect_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to External Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi_aspect.pkl']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both matrix and column names\n",
    "joblib.dump({\n",
    "    \"matrix\": low_combined,\n",
    "    \"columns\": column_names\n",
    "}, \"single_aspect.pkl\")\n",
    "\n",
    "joblib.dump({\n",
    "    \"matrix\": high_combined,\n",
    "    \"columns\": column_names\n",
    "}, \"multi_aspect.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../cleaning/multi_aspect.pkl\"\n",
    "\n",
    "data    = joblib.load(PATH)\n",
    "matrix  = data[\"matrix\"]\n",
    "columns = data[\"columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109266, 35515)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../cleaning/single_aspect.pkl\"\n",
    "\n",
    "data    = joblib.load(PATH)\n",
    "matrix  = data[\"matrix\"]\n",
    "columns = data[\"columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104247, 35515)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
